# Introduction

As data scientists, we are often driven towards those domains which generate vast amounts of data.  Social media, y, and z are all areas teeming with potential.  High-energy physics data is no exception.  The Large Hadron Collider (LHC) alone produces around 90 petabytes of data per year (roughly 240 terabytes per day)!  As such, there are thousands upon thousands of researchers combing through the LHC’s particle interactions to draw conclusions.  But, there exists one major difficulty in doing so:  the colliders themselves only have instruments that can detect physical quantities (energies, momentums, and the like).  As of yet, there exists no technology which can just magically detect the presence of various subatomic particles.  Any classification of particles must be done after the fact by researchers and their algorithms.

Different teams have different algorithms that they employ based on their specific domains of interest.  For instance, this past year we have completed work on replicating a leading neural network which determines if a particle jet contains a Higgs Boson.  In an ideal world, there would be a multiclass classifier which could perfectly discriminate between all types of jets which may occur.  While a similar classifier exists at the LHC, it is obviously not perfect.  Further, the ceiling for improvement extends higher with each advancement in machine learning- deep neural network architecture being the most recent.

The question then becomes, how well could our own model perform? That is, in implementing our own neural network, could we achieve a high level of model performance?

#### Pipeline
(high level view of how our project was completed)

# Our Process: (maybe toggle tabs or which is shown?)
On the whole, we thought it best to approach this task from multiple perspectives.  Our previous work (link to a pdf of our final paper from last year) employed a one-dimensional convolutional neural network.  During our process of building that model, we became more aware of other architectures which existed which could perhaps be better suited to our problem at hand.  Thus, we set about investigating and implementing the following architectures:

#### CNN
Explain what it is, and our history with it.
#### ENN
Explain what it is, and how our efforts with it faltered
#### GNN
Explain what it is, etc


# Impact:
#### Results
(more reporting on what we got)
Image of our performance

#### Impact
(how what we got matters)



<!--- You can use the [editor on GitHub](https://github.com/sharmimathur/multiclassification-particle-jets/edit/gh-pages/README.md) to maintain and preview the content for your website in Markdown files. -->

<!--- Whenever you commit to this repository, GitHub Pages will run [Jekyll](https://jekyllrb.com/) to rebuild the pages in your site, from the content in your Markdown files. -->

<!--- ### Markdown -->

<!--- Markdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for -->

<!--- - Bulleted -->
<!--- - List -->

<!--- 1. Numbered -->
<!--- 2. List -->

<!--- **Bold** and _Italic_ and `Code` text -->

<!--- [Link](url) and ![Image](src) -->

<!--- For more details see [GitHub Flavored Markdown](https://guides.github.com/features/mastering-markdown/). --->

<!--- ### Jekyll Themes -->

<!--- Your Pages site will use the layout and styles from the Jekyll theme you have selected in your [repository settings](https://github.com/sharmimathur/multiclassification-particle-jets/settings). The name of this theme is saved in the Jekyll `_config.yml` configuration file. --->

<!--- ### Support or Contact --->

<!--- Having trouble with Pages? Check out our [documentation](https://docs.github.com/categories/github-pages-basics/) or [contact support](https://support.github.com/contact) and we’ll help you sort it out. --->
